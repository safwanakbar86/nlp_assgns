{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Model Perplexity:\n",
      "Positive Test: 653.8657692515221\n",
      "Negative Test: 636.8014658788888\n",
      "\n",
      "Smoothed Unigram Model Perplexity:\n",
      "Positive Test: 656.2273466083091\n",
      "Negative Test: 639.4011766176395\n",
      "\n",
      "Bigram Model Perplexity:\n",
      "Positive Test: 47.80126358140742\n",
      "Negative Test: 49.02762851428299\n",
      "\n",
      "Smoothed Bigram Model Perplexity:\n",
      "Positive Test: 1417.7376675989808\n",
      "Negative Test: 1436.161940495924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "UNK = \"UNK\"\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "# Read file into corpus\n",
    "def readFileToCorpus(f):\n",
    "    if os.path.isfile(f):\n",
    "        with open(f, \"r\") as file:\n",
    "            corpus = [[START] + line.strip().split() + [END] for line in file]\n",
    "        return corpus\n",
    "    else:\n",
    "        print(f\"Error: corpus file {f} does not exist\")\n",
    "        sys.exit()\n",
    "\n",
    "# Preprocess the corpus\n",
    "def preprocess(corpus):\n",
    "    freqDict = defaultdict(int)\n",
    "    for sen in corpus:\n",
    "        for word in sen:\n",
    "            freqDict[word] += 1\n",
    "\n",
    "    for sen in corpus:\n",
    "        for i in range(len(sen)):\n",
    "            if freqDict[sen[i]] < 2:\n",
    "                sen[i] = UNK\n",
    "    return corpus\n",
    "\n",
    "# Preprocess test corpus\n",
    "def preprocessTest(vocab, corpus):\n",
    "    for sen in corpus:\n",
    "        for i in range(len(sen)):\n",
    "            if sen[i] not in vocab:\n",
    "                sen[i] = UNK\n",
    "    return corpus\n",
    "\n",
    "def generateSentencesToFile(model, filename, num_sentences=20):\n",
    "    with open(filename, \"w\") as file:\n",
    "        for _ in range(num_sentences):\n",
    "            sentence = model.generateSentence()\n",
    "            file.write(sentence + \"\\n\")\n",
    "\n",
    "# Language Model Base Class\n",
    "class LanguageModel:\n",
    "    def generateSentence(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def getSentenceProbability(self, sen):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def getCorpusPerplexity(self, corpus):\n",
    "        log_prob = 0\n",
    "        total_words = 0\n",
    "        for sen in corpus:\n",
    "            prob = self.getSentenceProbability(sen)\n",
    "            if prob > 0:\n",
    "                log_prob += math.log(prob)\n",
    "                total_words += len(sen) - 1\n",
    "        return math.exp(-log_prob / total_words) if total_words > 0 else float('inf')\n",
    "\n",
    "# Unigram Model\n",
    "class UnigramModel(LanguageModel):\n",
    "    def __init__(self, corpus):\n",
    "        self.counts = defaultdict(int)\n",
    "        self.total = 0\n",
    "        for sen in corpus:\n",
    "            for word in sen:\n",
    "                self.counts[word] += 1\n",
    "                self.total += 1\n",
    "\n",
    "    def getSentenceProbability(self, sen):\n",
    "        prob = 1.0\n",
    "        for word in sen[1:]:\n",
    "            prob *= self.counts[word] / self.total\n",
    "        return prob\n",
    "    \n",
    "    def generateSentence(self):\n",
    "        sentence = [START]\n",
    "        while sentence[-1] != END:\n",
    "            next_word = random.choices(list(self.counts.keys()), weights=self.counts.values())[0]\n",
    "            sentence.append(next_word)\n",
    "        return \" \".join(sentence[1:-1])  # Remove <s> and </s>\n",
    "\n",
    "# Smoothed Unigram Model\n",
    "class SmoothedUnigramModel(UnigramModel):\n",
    "    def getSentenceProbability(self, sen):\n",
    "        vocab_size = len(self.counts)\n",
    "        prob = 1.0\n",
    "        for word in sen[1:]:\n",
    "            prob *= (self.counts[word] + 1) / (self.total + vocab_size)\n",
    "        return prob\n",
    "    \n",
    "    def generateSentence(self):\n",
    "        sentence = [START]\n",
    "        while sentence[-1] != END:\n",
    "            next_word = random.choices(list(self.counts.keys()), weights=self.counts.values())[0]\n",
    "            sentence.append(next_word)\n",
    "        return \" \".join(sentence[1:-1])  # Remove <s> and </s>\n",
    "\n",
    "# Bigram Model\n",
    "class BigramModel(LanguageModel):\n",
    "    def __init__(self, corpus):\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        for sen in corpus:\n",
    "            for i in range(len(sen) - 1):\n",
    "                self.unigram_counts[sen[i]] += 1\n",
    "                self.bigram_counts[sen[i]][sen[i+1]] += 1\n",
    "\n",
    "    def getSentenceProbability(self, sen):\n",
    "        prob = 1.0\n",
    "        for i in range(len(sen) - 1):\n",
    "            if self.unigram_counts[sen[i]] > 0:\n",
    "                prob *= self.bigram_counts[sen[i]][sen[i+1]] / self.unigram_counts[sen[i]]\n",
    "            else:\n",
    "                return 0.0\n",
    "        return prob\n",
    "    \n",
    "    def generateSentence(self):\n",
    "        sentence = [START]\n",
    "        while sentence[-1] != END:\n",
    "            prev_word = sentence[-1]\n",
    "            if prev_word in self.bigram_counts:\n",
    "                next_word = random.choices(\n",
    "                    list(self.bigram_counts[prev_word].keys()),\n",
    "                    weights=self.bigram_counts[prev_word].values()\n",
    "                )[0]\n",
    "            else:\n",
    "                next_word = END  # If no bigram exists, end the sentence\n",
    "            sentence.append(next_word)\n",
    "        return \" \".join(sentence[1:-1])  # Remove <s> and </s>\n",
    "\n",
    "# Smoothed Bigram Model\n",
    "class SmoothedBigramModel(BigramModel):\n",
    "    def getSentenceProbability(self, sen):\n",
    "        vocab_size = len(self.unigram_counts)\n",
    "        prob = 1.0\n",
    "        for i in range(len(sen) - 1):\n",
    "            prob *= (self.bigram_counts[sen[i]][sen[i+1]] + 1) / (self.unigram_counts[sen[i]] + vocab_size)\n",
    "        return prob\n",
    "    \n",
    "    def generateSentence(self):\n",
    "        sentence = [START]\n",
    "        while sentence[-1] != END:\n",
    "            prev_word = sentence[-1]\n",
    "            if prev_word in self.bigram_counts:\n",
    "                next_word = random.choices(\n",
    "                    list(self.bigram_counts[prev_word].keys()),\n",
    "                    weights=self.bigram_counts[prev_word].values()\n",
    "                )[0]\n",
    "            else:\n",
    "                next_word = END  # If no bigram exists, end the sentence\n",
    "            sentence.append(next_word)\n",
    "        return \" \".join(sentence[1:-1])  # Remove <s> and </s>\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    trainCorpus = preprocess(readFileToCorpus('train.txt'))\n",
    "    vocab = set(word for sen in trainCorpus for word in sen)\n",
    "    posTestCorpus = preprocessTest(vocab, readFileToCorpus('pos_test.txt'))\n",
    "    negTestCorpus = preprocessTest(vocab, readFileToCorpus('neg_test.txt'))\n",
    "    \n",
    "    models = {\n",
    "        \"Unigram\": UnigramModel(trainCorpus),\n",
    "        \"Smoothed Unigram\": SmoothedUnigramModel(trainCorpus),\n",
    "        \"Bigram\": BigramModel(trainCorpus),\n",
    "        \"Smoothed Bigram\": SmoothedBigramModel(trainCorpus)\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"{name} Model Perplexity:\")\n",
    "        print(f\"Positive Test: {model.getCorpusPerplexity(posTestCorpus)}\")\n",
    "        print(f\"Negative Test: {model.getCorpusPerplexity(negTestCorpus)}\\n\")\n",
    "\n",
    "    unigram = UnigramModel(trainCorpus)\n",
    "    smooth_unigram = SmoothedUnigramModel(trainCorpus)\n",
    "    bigram = BigramModel(trainCorpus)\n",
    "    smooth_bigram = SmoothedBigramModel(trainCorpus)\n",
    "\n",
    "    generateSentencesToFile(unigram, \"unigram_output.txt\")\n",
    "    generateSentencesToFile(smooth_unigram, \"smooth_unigram_output.txt\")\n",
    "    generateSentencesToFile(bigram, \"bigram_output.txt\")\n",
    "    generateSentencesToFile(smooth_bigram, \"smooth_bigram_kn_output.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.\n",
    "The sentence length is typically determined by the probability of encountering an end-of-sentence token (`</s>`) in unigrams. Unigrams consider each word independently without context. Bigrams determine the next word based on the previous word, leading to more structured sentences. This control sentences length more naturally as the likelihood of `</s>` depends on the preceding word and not randomly.\n",
    "\n",
    "#### 2.\n",
    "Yes because:\n",
    "- The unigram model considers independence between words leading to less structured sentences.\n",
    "- The bigram model assumes context influencing word selection resulting in more realistic sentences.\n",
    "- The smoothed bigram model adjusts probabilities to handle unseen word pairs altering likelihood of sentences.\n",
    "\n",
    "#### 3.\n",
    "The bigram model generally produces more coherent sentences than the unigram model because of word order and context. The smoothed bigram model tends to introduce more randomness due to smoothing leading to unnatural word pairings.\n",
    "\n",
    "#### 4.\n",
    "- **Unigram Model Perplexity:**\n",
    "  - Positive Test: **653.87**\n",
    "  - Negative Test: **636.80**\n",
    "\n",
    "- **Smoothed Unigram Model Perplexity:**\n",
    "  - Positive Test: **656.23**\n",
    "  - Negative Test: **639.40**\n",
    "\n",
    "- **Bigram Model Perplexity:**\n",
    "  - Positive Test: **47.80**\n",
    "  - Negative Test: **49.03**\n",
    "\n",
    "- **Smoothed Bigram Model Perplexity:**\n",
    "  - Positive Test: **1417.74**\n",
    "  - Negative Test: **1436.16**\n",
    "\n",
    "For the unigram and smoothed unigram models, the positive test corpus has a slightly higher perplexity. This suggests that the word distribution in the positive test set differs more from the training set. For the bigram and smoothed bigram models, the negative test corpus has a slightly higher perplexity than the positive test corpus. This suggests that smoothing introduces a greater degree of uncertainty, making it less effective in predicting test data compared to the regular bigram model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
